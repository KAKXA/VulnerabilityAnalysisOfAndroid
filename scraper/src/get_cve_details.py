import asyncio

import pandas as pd
from bs4 import BeautifulSoup
from config import *
from scraper.src.utils.get_html import getHtml, getHtmlAsync


def getCveDetails(htmlFinish: bool):
    if not os.path.exists(cveDetailsPageDirectory):
        os.mkdir(cveDetailsPageDirectory)
    if not os.path.exists(cveDetailsDirectory):
        os.mkdir(cveDetailsDirectory)

    pageNum = 85
    urlAndLocalPath = []
    for i in range(1, pageNum + 1):
        url = cveDeatilsUrlPrefix + str(i) + cveDeatilsUrlSuffix
        localPath = os.path.join(cveDetailsPageDirectory, str(i) + ".html")

        html = getHtml(url, localPath)
        for cveUrl in re.findall("/cve/" + cvePattern, html):
            cveLocalPath = os.path.join(cveDetailsDirectory, cveUrl.split("/")[-1] + ".html")
            urlAndLocalPath.append((cveDetailsDomain + cveUrl, cveLocalPath))

    if not htmlFinish:
        step = 20
        tasks = [
            asyncio.ensure_future(getOneCveDetailHtml(urlAndLocalPath, i, step))
            for i in range(step)
        ]
        loop = asyncio.get_event_loop()
        loop.run_until_complete(asyncio.wait(tasks))

    res = []
    cnt = 0
    for url, localPath in urlAndLocalPath:
        try:
            res.append(getOneCveDetail(url.split("/")[-1]))
        except Exception as e:
            print(localPath)
            raise e

        cnt += 1
        if cnt % 100 == 0:
            print(str(cnt) + " cves is finished.")

    with open(cveDetailsJson, "w") as fp:
        json.dump(res, fp)


async def getOneCveDetailHtml(urlAndLocalPath, startIndex, step):
    cnt = 0
    for i in range(startIndex, len(urlAndLocalPath), step):
        html = await getHtmlAsync(urlAndLocalPath[i][0], urlAndLocalPath[i][1])
        cnt += 1
        print("task " + str(startIndex) + ": " + str(cnt))


def getOneCveDetail(cveId) -> dict:
    url = cveDetailsDomain + '/cve/' + cveId
    localPath = os.path.join(cveDetailsDirectory, cveId + '.html')

    res = {"cveId": cveId}

    html = getHtml(url, localPath)
    bs = BeautifulSoup(html, "lxml")
    try:
        summaryAndDate = bs.find("div", {"class": "cvedetailssummary"}).getText()
    except AttributeError:
        print(cveId + ' does not exist in cvedetails.com')
        return {}
    summary = re.search(".*?Publish", summaryAndDate, re.S).group().strip("Publish").strip()
    publishDate = re.search(
        r'\d{4}-\d{1,2}-\d{1,2}',
        re.search(r"Publish Date.*?\d{4}-\d{1,2}-\d{1,2}", summaryAndDate, re.S).group(),
        re.S,
    ).group()
    items = pd.read_html(localPath, attrs={"id": "cvssscorestable"})[0].values
    for item in items:
        res[item[0]] = item[1]

    try:
        refs = pd.read_html(localPath, attrs={"id": "vulnrefstable"})[0].values
        res["refs"] = []
        for ref in refs:
            ref = list(ref)
            res["refs"].append(ref[0])
    except ValueError:
        print(cveId + ' does not have references in cvedetails.com.')

    res["summary"] = summary
    res["publishDate"] = publishDate

    return res


if __name__ == "__main__":
    getCveDetails(htmlFinish=True)
