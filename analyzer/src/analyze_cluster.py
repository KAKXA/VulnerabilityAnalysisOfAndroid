from analyzer.src.analyze_cluster import *
from sklearn.feature_extraction.text import TfidfVectorizer
from config import *
from cleaner.src.utils.load import loadColumn

def analyse():
    cveList = loadColumn(afterFillCveSummaryJson, 'summary')
    summaryClustering(cveList)

def summaryClustering(summaries):

    # 加载文本数据
    from time import time
    print("loading documents ...")
    t = time()

    # 掐头去尾
    for i, doc in enumerate(summaries):
        summaries[i] = re.sub(r'\d', '', doc)

    # 文本向量化表示
    max_features = 20000
    print("vectorizing documents ...")
    t = time()
    vectorizer = TfidfVectorizer(max_df=0.4, min_df=2, max_features=max_features, encoding='latin-1')
    X = vectorizer.fit_transform((d for d in summaries))

    # 文本聚类
    from sklearn.cluster import KMeans, MiniBatchKMeans
    print("clustering documents ...")
    t = time()
    n_clusters = 50
    kmean = MiniBatchKMeans(n_clusters=n_clusters, max_iter=100, tol=0.01, verbose=1, n_init=3)
    kmean.fit(X)
    print("kmean: k={}, cost={}".format(n_clusters, int(kmean.inertia_)))
    print("done in {0} seconds".format(time() - t))

    # 打印实例数量
    print("实例总数 = ", len(kmean.labels_))

    # 打印每个簇的前10个显著特征
    print("每个簇的前10个显著特征：")
    order_centroids = kmean.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    for i in range(n_clusters):
        print("Cluster %d:" % i, end='')
        for ind in order_centroids[i, :10]:
            print(' %s' % terms[ind], end='')
        print()

if __name__ == '__main__':
    analyse()