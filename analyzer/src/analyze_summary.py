from time import time

import numpy as np
import pandas as pd
from analyzer.src.analyze_cluster import *
from config import *
from sklearn.feature_extraction.text import TfidfVectorizer


def analyzeSummary(cveList):
    # input: summary list
    # return {c1terms str: all c1 texts, c2..., cx}
    summaries = []
    key = "summary"
    # stopword
    for cveDict in cveList:
        if cveDict[key]:
            wordList = cveDict[key]

            newWordList = []
            for word in wordList:
                if word not in generalStopWords:
                    newWordList.append(word)
            cveDict["summary"] = newWordList
            summaries.append(" ".join(cveDict[key]))
    cluster(summaries, 6, os.path.join(cwd, "1.csv"))

    print()
    summaries = []
    for cveDict in cveList:
        if cveDict[key]:
            wordList = cveDict[key]

            newWordList = []
            for word in wordList:
                if word not in summaryStopWords:
                    newWordList.append(word)
            cveDict["summary"] = newWordList
            summaries.append(" ".join(cveDict[key]))
    cluster(summaries, 6, os.path.join(cwd, "2.csv"))


def cluster(summaries, n_clusters, csvFilePath):
    t = time()

    # 文本向量化表示
    max_features = 20000
    t = time()
    vectorizer = TfidfVectorizer(
        max_df=0.4, min_df=2, max_features=max_features, encoding="latin-1"
    )
    X = vectorizer.fit_transform((d for d in summaries))

    # 文本聚类
    from sklearn.cluster import KMeans

    t = time()
    kmean = KMeans(n_clusters=n_clusters, max_iter=100, tol=0.01, verbose=0, n_init=3)
    kmean.fit(X)

    # df = pd.DataFrame()
    # df['dataIndex'] = summaries
    # df['cluster'] = kmean.labels_
    # print(df[df.cluster == 3])
    # print(df.iloc[3])
    # print(clusterIndicesNumpy(2, kmean.labels_))
    # print("kmean聚类中心\n", (kmean.cluster_centers_))
    # quantity = pd.Series(kmean.labels_).value_counts()
    # print("cluster2聚类数量\n", (quantity))
    # # 获取聚类之后每个聚类中心的数据
    # res0Series = pd.Series(kmean.labels_)
    # res0 = res0Series[res0Series.values == 1]
    # print("类别为1的数据\n", (df.iloc[res0.index]))

    order_centroids = kmean.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    termsList = []
    res = {}
    table = [list() for j in range(6)]
    for i in range(n_clusters):
        # print("Cluster %d:" % i, end="")
        # print("Cluster " + str(i) + ":", end="")
        for ind in order_centroids[i, :10]:
            termsList.append(terms[ind])
            print("\t%s" % terms[ind], end="")
            table[i].append(terms[ind])
        print()
        key = " ".join(termsList)
        val = list(clusterIndicesNumpy(i, kmean.labels_))
        res[key] = val
    import csv

    table = list(zip(*table))
    table.insert(0, ['类' + str(i) for i in range(1, 7)])
    with open(csvFilePath, "w", newline="") as fp:
        writer = csv.writer(fp)
        for r in table:
            writer.writerow(r)

    return res


def clusterIndicesNumpy(clustNum, labels_array):  # numpy
    return np.where(labels_array == clustNum)[0]


def clusterIndicesComp(clustNum, labels_array):  # list comprehension
    return np.array([i for i, x in enumerate(labels_array) if x == clustNum])


if __name__ == "__main__":
    from cleaner.src.utils.load import loadAll

    cveList = loadAll(jsonForAnalyze)
    print(analyzeSummary(cveList))
