import datetime
from config import *
from bs4 import BeautifulSoup
from scraper.src.utils.get_html import getHtml
from scraper.src.utils.get_googlesource_utils.convertor import str2Pat, googlesourceUrl2FileName

def analyzeLineChange(cveList):
    cveAndUrls = {}
    for cveDict in cveList:
        cveId = cveDict["cveId"]
        """
        {$id: {'files':{$branchAndPath: {$allCodes in the frame}}}, 'autherTime': $autherTime, 'committerTime': $committerTime}}
        """
        urlsPaths = dict()
        refs = cveDict["references"]
        if refs:
            for ref in refs:
                # s.add(re.match(r'.*\.(com|org|net|cz|name|info|fi|edu|io|ca|nl|se)', ref).group())
                url = re.search(r"https://android\.googlesource\.com\S*", ref)
                if url:
                    url = url.group().rstrip("/")
                    trueUrl = re.sub(r"%5E%21|%5E!|\^!|\^%21", "", url)

                    if re.search(r"\.java$", trueUrl):
                        continue
                    localPath = os.path.join(
                        googlesourceDirectory, googlesourceUrl2FileName(trueUrl)
                    )
                    urlsPaths[trueUrl] = localPath
                    getHtml(trueUrl + "^!", localPath)
        if urlsPaths:
            cveAndUrls[cveId] = urlsPaths

    for cveId, urlsPaths in cveAndUrls.items():
        changes = getChangesOfOneCve(cveId, urlsPaths)
        with open('happy.json', 'w') as fp:
            json.dump(changes, fp)
        ####################################
        return
        ####################################


def getChangesOfOneCve(cveId, urlsPaths):
    changes = []
    for url, localPath in urlsPaths.items():
        html = getHtml(url, localPath)
        print(localPath, url)
        branchNotClean = re.sub(r"https://android.googlesource.com/|/\+.*$", "", url)
        branch = re.sub(r"%2F", "/", branchNotClean)
        commitId = url.split("/")[-1]
        change = getOneGoogleSourcePage(html, localPath, branch)
        changes.append(change)
    return changes

def getOneGoogleSourcePage(html: str, localPath: str, branch: str):
    # resKeys = [
        # 'authorTime',
        # 'committerTime', 
        # 'branch', 
        # 'filesAndChanges'
    # ]
    res = dict()

    bs = BeautifulSoup(html, "lxml")
    bs.findChild("table")
    bs.findChild("tr")
    tds = bs.findAll("td")

    authorAndCommitterTime = []
    for td in tds:
        try:
            time = datetime.datetime.strptime(td.getText(), "%a %b %d %H:%M:%S %Y %z")
        except Exception as e:
            continue
        authorAndCommitterTime.append(time)
    
    if len(authorAndCommitterTime) != 2:
        raise ValueError(localPath)

    fileNames = list(re.finditer("--- a/(.*)", html))
    filesAndChanges = {}
    for i, tmpFileName in enumerate(fileNames):
        fileName = tmpFileName.group(1)
        if i == len(fileNames) - 1:
            pat = str2Pat(fileName) + "(.*)$"
        else:
            pat = str2Pat(fileName) + "(.*?)" + str2Pat(fileNames[i + 1].group(1))

        subHtml = re.search(pat, html, re.S).group(1)
        modifyList = getModifyInOneFile(subHtml)
        filesAndChanges[fileName] = modifyList

    res['authorTime'] = str(authorAndCommitterTime[0])
    res['committerTime'] = str(authorAndCommitterTime[1])
    res['branch'] = branch
    res['fileNamesAndChanges'] = filesAndChanges

    return res

def getModifyInOneFile(subHtml) -> list:
    modify = []
    for insertAndDelete in re.finditer(r'">(-|\+)(.*)</span>', subHtml):
        content = insertAndDelete.group(2).strip()
        if content:
            modify.append([insertAndDelete.group(1), content])
    return modify